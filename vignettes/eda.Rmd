---
title: "Exploratory Data Analysis"
subtitle: Adapted for [ForestGEO](https://twitter.com/ForestGEO?lang=en) from [R for Data Science](http://r4ds.had.co.nz/), by [Hadley Wickham](https://twitter.com/hadleywickham) and [Garrett Grolemund](https://twitter.com/StatGarrett)
author: "Mauro Lepore"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Exploratory Data Analysis}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = TRUE,
  comment = "#>",
  fig.show = "hold",
  out.width = "98%", 
  fig.width = 9.5, 
  fig.asp = 0.7
)
```

# Introduction

This vignette literaly reproduces the section Exploratory Data Analysis in R for Data Science, by Hadley Wickham and Garrett Grolemund, except that the examples and some text were changed to more strongly engage ecologysts from ForestGEO.

You will learn how to explore your data systematically, using modern and powerful tools for visualization and transformation. An exploratory data analysis is an iterative process that aims to use data and some research questions to learn something that can help you refine those questions and move forward the learning spiral.

## Prerequisites

Most functions we will use are available in the __tidyverse__ package (https://www.tidyverse.org/). If you are new to the tidyverse, you may feel a little frustrated at the beginning, but quickly your effort will pay-off. The tools in the tidyverse are powerful, relatively easy to use and consistent; so once you learn some, you will learn most other tools intuitively. To more completely learn how to do data science with the tidyverse read R for Data Sciencev (http://r4ds.had.co.nz/).

```{r, eval=TRUE}
library(tidyverse)  # visualize and transform data (and more)
```

The data we will use is a public dataset of trees censused in a forest plot in Luquillo, in Puerto Rico. Those data are available in the data set luquillo_tree6_random in the data-package __fgeo.data__.

```{r, eval=TRUE}
library(fgeo.data)
# Convenient nickname
# TODO Replace with stem everywhere
stem <- luquillo_stem_random
tree <- luquillo_tree6_random
tree
```

## Style

### The tidyverse style guide

I use The tidyverse style guide (http://style.tidyverse.org/).

> Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread.

-- [The tidyverse style guide](http://style.tidyverse.org/)

### Combining multiple operations with the pipe %>%

Often I combine multiple operations with the pipe %>% because it makes the code considerably more readable.

> [The pipe] focuses on the transformations, not what's being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce %>% when reading code is "then".

> Behind the scenes, x %>% f(y) turns into f(x, y), and x %>% f(y) %>% g(z) turns into g(f(x, y), z) and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom.

> Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered.

–- [R for Data Science](http://r4ds.had.co.nz/transform.html)

# Questions and Definitions

## Questions

To start exploring your data, you can generally ask these questions:

* What type of variation occurs within my variables?
* What type of covariation occurs between my variables?

## Definitions

A __variable__ is a quantity, quality, or property that you can measure.

A __value__ is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.

An __observation__ is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. I’ll sometimes refer to an observation as a data point.

__Tabular data__ is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.

__Variation__ is the tendency of the values of a variable to change from measurement to measurement.

# Variation

Every variable varies with a particular pattern, and that pattern may be insightful. To understand the variation pattern of a variable we can visualize the distribution of the variables' values. The best way to visualize a variable’s distribution depends on whether the variable is categorical or continuous.

The `tree` dataset has both, categorical and continuous variables. In R, categorical variables are usually stored as character strings (<chr>) or factors (<fctr>), and continuous variables are stored as integers (<int>) or doubles (<dbl>).

```{r, eval=TRUE}
tree
```

Notice that `tree` is a tibble -- a subclass of dataframe optimized to handle large data sets, which prints more information and nicer.

```{r}
class(tree)
```

By default, tibbles print a few rows only; to print more rows use:

```R
print(<YOUR_TIBBLE>, n = <N_ROWS>, width = <N_COLUMNS>)
```

You can also write a helper funciton to `print_all()` rows and columns (as `data.frame()`).

```{r, eval=TRUE}
# or print all rows in a tibble with
print_all <- function(x) {
  print(x, n = nrow(x), width = Inf)
}

# Example
tbl <- tibble(x = runif(21))
tbl
print_all(tbl)
```

For alternative views try:

```R
# Output is too long and doesn't print nicely
as.data.frame(tree)  
```

```{r}
# informative and prints nice
str(tree)  
# like str() but shows as much data as possible
glimpse(tree)  
```

If you are in RStudio, your best option might be `View()`, which lets you search values and filter columns.

```R
View(tree)           
```

<img src="https://user-images.githubusercontent.com/5856545/41784224-12a3adca-760d-11e8-8b80-50d8775f01db.png" align="right" height=160 /> 


## Visualizing Distributions

### Categorical variables

> A variable is categorical if it can only take one of a small set of values. To examine the distribution of a categorical variable, use a bar chart.

```{r}
ggplot(data = tree) +
  geom_bar(mapping = aes(x = status))
```

The categories A, D and M, of the variable `status` mean "alive", "dead" and "missing" (http://ctfs.si.edu/Public/DataDict/data_dict.php).

```{r}
data_dictionary %>% 
  filter(column == "Status") %>% 
  pull(description)
```

### Continuous Variables

> A variable is continuous if it can take any of an infinite set of ordered values. To examine the distribution of a continuous variable use a histogram. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns.

Let’s explore `dbh`, which represents the stem diameter breast height. Now we will focus on small trees; we'll explore big trees later. And we will try bars of different widths and choose one for further analyses.

```{r}
small_dbh <- filter(tree, dbh < 300)

# Save data and mappings in "p" to reuse them later.
p <- ggplot(small_dbh, aes(x = dbh))
```

```{r}
p + geom_histogram(binwidth = 10)
```

```{r}
p + geom_histogram(aes(dbh), binwidth = 30)
```

```{r}
p + geom_histogram(aes(dbh), binwidth = 60)
```

A binwidth of 30 dbh seems useful.

```{r}
# Save to reuse
useful_binwidth <- 30
```

The code above uses the temporary variable p; this reduces the boilerplate code and helps you focus on important changes. Yet this technique must be used carefully. It is good practice to avoid uninformative temporary names like p unless the meaning is obvious from the context and the “life” of the temporary variable is clearly limited to a discrete code chunk. Generally, prefer long informative names over short uninformative ones (http://a.co/4f8df29).

You can reduce the boilerplate code even further and without temporary variables:

* Wite a custom function with an argument to vary binwidth.
* Use `purrr::map()` (or `lapply()`) to iteratively pass different binwidths to our custom function.

```{r, fig.align="default", out.width="30%", fig.widh=(6 * 0.3 / 0.95)}
# DRY principle: Don't Repeat Yourself
compare_bindiwdth <- function(binwidth) {
  ggplot(small_dbh, aes(x = dbh)) +
    geom_histogram(binwidth = binwidth)
}

bw <- c(10, 30, 60)
map(bw, compare_bindiwdth)
```

To overlay multiple histograms in the same plot, `geom_freqpoly()` may produce clearer plots than `geom_histogram()` because it is easier to understand overlying lines than bars.

```{r, fig.align="default", out.width="45%", fig.widh=(6 * 0.5 / 0.95)}
# Make n groups with ~ numbers of observations with `ggplot2::cut_number()`
small_cut_number <- mutate(small_dbh, equal_n = cut_number(dbh, n = 5))

p <- ggplot(data = small_cut_number, aes(x = dbh, color = equal_n)) +
  # use available space on plot area (defult plots legend outside)
  theme(
    legend.position = c(.95, .95),
    legend.justification = c("right", "top")
  )

# Left
p + geom_histogram()

# Right
p + geom_freqpoly()
```

### Typical and rare Values

In both bar charts and histograms, tall and short bars let us explore common and less-common values. For example, we could ask:

* Which values are the most common? Why?
* Which values are rare? Why? Does that match your expectations?
* Can you see any unusual patterns? What might explain them?

The later two plots give a good estimate of what tree diameters are most common. You can compute the same count manually, by cutting the variable dbh with `ggplot2::cut_width()` and then counting the unique pieces with `dplyr::count()`.

```{r}
small_dbh %>% 
  mutate(dbh_mm = cut_width(dbh, width = useful_binwidth)) %>% 
  count(dbh_mm, sort = TRUE)
```

The most common trees are those which diameter (`dbh`) is between 15 mm and 45 mm.

### Clustered values

In this section we will focus on the largest trees; later we will explore all trees, including medium-size ones.

```{r}
large_dbh <- tree %>% filter(dbh > 300)
```

Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask:

* How are the observations within each cluster similar to each other?
* How are the observations in separate clusters different from each other?
* How can you explain or describe the clusters?
* Why might the appearance of clusters be misleading?

The clustering, however, may be an artifact of the chosen binwidth:

```{r}
p <- ggplot(large_dbh, aes(dbh))

# Left: Are these clusters meaningful?
p + geom_histogram(binwidth = 25)

# Right: Is this a more meaningful representation?
p + geom_histogram(binwidth = 100)
```

### Unusual values

In this section we will work with the entire `tree` data set.

Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram.

For example, take the distribution of the `dbh` variable of the `tree` data set. Notice how wide are the limits of the x-axis.

```{r}
p <- ggplot(tree, aes(dbh)) +
  geom_histogram(binwidth = useful_binwidth)
p
```

There are so many observations in the common bins that the rare bins are so short that you can barely see them . To make the unusual values more noticable, we can zoom into smaller values of the y-axis with `coord_cartesian()`.

```{r}
p + coord_cartesian(ylim = c(0, 15))
```

`coord_cartesian()` also has an `xlim` argument for when you need to zoom into the x-axis. __ggplot2__ also has `xlim()` and `ylim()` functions that work slightly differently: they throw away the data outside the limits.

This allows us to see that `dbh` values over ~625 are unusual and may be outliers. We can pluck them out with `dplyr::filter()` and select a subset of informative variables:

```{r}
treshold <- 625

unusual <- tree %>% 
  filter(dbh > treshold ) %>% 
  select(treeID, ExactDate, status, gx, gy, dbh) %>%
  arrange(dbh)
unusual
```

You could plot the unusual trees to see where they are located.

```{r}
ggplot(tree, aes(gx, gy)) + 
    # `alpha` controls opacity
    geom_point(alpha = 1/10) +
    # highlight unusual trees
    geom_point(data = unusual, colour = "red")
```

It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.

# Missing Values

If you’ve encountered unusual values in your data set, and simply want to move on to the rest of your analysis, you have two options.

1. Drop the entire row with the strange values:

```{r}
are_usual <- !tree$treeID %in% unusual$treeID
usual <- filter(tree, are_usual)

# Confirm data set of usual trees has less rows than full data set.
nrow(tree)
nrow(usual)
```

I don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left!

2. Instead, I recommend replacing the unusual values with missing values. The easiest way to do this is to use `dplyr::mutate()` to replace the variable with a modified copy. You can use the `ifelse()` function to replace unusual values with `NA`:

```{r}
are_unusual <- !are_usual
with_unusual_made_NA <- tree %>% 
  mutate(dbh = ifelse(are_unusual, NA_real_, dbh))
```

`ifelse()` has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is `TRUE`, and the value of the third argument, no, when it is false.

```{r}
# Confirm no rows have been removed,
nrow(tree)
nrow(with_unusual_made_NA)

# but dbh of unusual trees is NA
unusual_only <- with_unusual_made_NA %>% 
  filter(are_unusual) %>% 
  select(dbh, treeID)
unusual_only
```

Alternatively to ifelse, use `dplyr::case_when()`. `case_when()` is particularly useful inside `mutate()` when you want to create a new variable that relies on a complex combination of existing variables.

Like R, __ggplot2__ subscribes to the philosophy that missing values should never silently go missing. It’s not obvious where you should plot missing values, so ggplot2 doesn’t include them in the plot, but it does warn that they’ve been removed (left plot below). To suppress that warning, set `na.rm = TRUE` (right plot below).

```{r, fig.align="default", out.width="50%", fig.widh=(6 * 0.5 / 0.95)}
# Left: don't remove NAs and get a warning on the console
ggplot(unusual_only, aes(dbh)) + 
  geom_histogram(binwidth = useful_binwidth) +
  labs(title = "Expect empty plot but get a warning")

# Right: NAs explicitely removed in geom_histogram(), so no warning
ggplot(unusual_only, aes(dbh)) + 
  geom_histogram(binwidth = useful_binwidth, na.rm = TRUE) +
  labs(title = "Expect empty plot but no warning")
```

Other times you want to understand what makes observations with missing values different to observations with recorded values.

For example, in `tree`, some missing values in the `dbh` variable correspond to alive trees. So you might want to compare the `DFstatus` for trees with missing and non-missing values of `dbh`. You can do this by making a new variable with `is.na()`.

```{r}
missing_dbh <- tree %>% 
  mutate(missing = is.na(dbh))

ggplot(missing_dbh) + 
  geom_bar(aes(DFstatus, fill = missing))
```

## Exercises

1. What happens to missing values in a histogram? What happens to missing values in a bar chart? Why is there a difference?

2. What does na.rm = TRUE do in mean() and sum()?

# Covariation

If variation describes the behavior within a variable, co-variation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot co-variation is to visualize the relationship between two or more variables. How you do that should again depend on the type of variables involved.

## A categorical and continuous variable

It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. For example, let’s explore how stem diameter (dbh) varies with species (sp):

TODO xxx cont. here http://rpubs.com/forestgeo/eda at Covariaiton
TODO Replace tree with stem 
